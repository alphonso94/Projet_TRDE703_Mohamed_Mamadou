

## üèóÔ∏è Note d'Architecture : Datamart Open Food Facts

### 1. Choix Techniques (Stack)

* **Moteur de calcul : Apache Spark (PySpark)** : Choisi pour sa capacit√© √† traiter des volumes massifs (le fichier complet Open Food Facts fait plusieurs Go) de mani√®re distribu√©e et pour sa gestion native des sch√©mas complexes.
* **Stockage : MySQL** : Utilis√© comme cible pour le Datamart en raison de sa compatibilit√© parfaite avec les outils de BI comme Tableau et sa gestion robuste des relations (cl√©s √©trang√®res).
* **Connectivit√© : JDBC & ODBC** : Utilisation du connecteur JDBC MySQL pour l'√©criture depuis Spark et du pilote ODBC pour la lecture dans Tableau, garantissant une communication standardis√©e.

### 2. Mod√©lisation : Sch√©ma en √âtoile (Star Schema)

Nous avons opt√© pour une mod√©lisation en √©toile car elle est optimis√©e pour l'analyse d√©cisionnelle.

* **Table de Faits (`fact_nutrition_snapshot`)** : Stocke les mesures quantitatives (calories, nutriments) et les scores. Elle est l√©g√®re car elle ne contient que des cl√©s num√©riques et des valeurs d√©cimales.
* **Tables de Dimensions (`dim_product`, `dim_brand`, etc.)** : Contiennent les attributs descriptifs. Cela permet de filtrer ou de regrouper les donn√©es sans dupliquer le texte lourd dans la table de faits.

### 3. Strat√©gie d'Upsert (Update + Insert)

C'est la m√©thode de gestion des mises √† jour des produits.

* **Probl√©matique** : Un produit Open Food Facts peut √™tre modifi√© plusieurs fois dans le temps (nouvelle recette, nouveau packaging).
* **Solution choisie : D√©doublonnage √† la source** :
1. On utilise une **Window Function** (`partitionBy("code")`) ordonn√©e par la date de modification (`last_modified_t`).
2. On ne conserve que la ligne la plus r√©cente (`rn=1`).
3. Lors de l'√©criture dans MySQL, le mode `truncate` assure un rafra√Æchissement complet (Full Reload) pour garantir la coh√©rence des cl√©s primaires.



### 4. Flux de donn√©es (Data Flow)

1. **Bronze (Raw)** : Lecture du CSV brut avec `inferSchema`.
2. **Silver (Conformed)** : Nettoyage, normalisation des types (Decimal, Long), et d√©doublonnage technique.
3. **Gold (Datamart)** : Jointures avec les dimensions pour r√©cup√©rer les cl√©s √©trang√®res (`SK`) et calcul du score de compl√©tude.

---

### Pourquoi cette architecture est-elle robuste ?

* **√âvolutivit√©** : Si vous passez de 1 000 √† 1 million de lignes, Spark g√©rera la charge sans modification du code.
* **Qualit√©** : Chaque √©tape est journalis√©e dans la table `etl_metrics_logs`, permettant un audit rapide des rejets.
* **Performance Tableau** : Gr√¢ce au sch√©ma en √©toile et aux index SQL, les tableaux de bord se chargeront beaucoup plus vite qu'avec un seul gros fichier plat.

**Voulez-vous que je vous aide √† dessiner le sch√©ma DDL (le code SQL de cr√©ation des tables) correspondant exactement √† ce sch√©ma en √©toile ?**