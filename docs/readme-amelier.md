Voici une proposition de **README** structur√©e et professionnelle pour votre projet, r√©dig√©e selon les exigences de rigueur du niveau M1.

---

# OFF Nutrition & Quality - ETL Pipeline (PySpark)

## üìå Pr√©sentation du Projet

Ce projet consiste en la mise en place d'une cha√Æne d'int√©gration de donn√©es de bout en bout √† partir des donn√©es **OpenFoodFacts (OFF)**. L'objectif est de transformer des donn√©es massives brutes (Datalake) en un **Datamart MySQL** mod√©lis√© en √©toile pour r√©pondre √† des probl√©matiques m√©tier sur la qualit√© nutritionnelle.

## üèóÔ∏è Architecture Technique

Le pipeline suit une architecture de donn√©es en couches (Medallion Architecture):

* 
**Bronze (Ingestion)** : Lecture des donn√©es brutes JSON/CSV avec sch√©ma explicite (pas d'inf√©rence).


* 
**Silver (Conformation)** : Normalisation des types, d√©doublonnage par code-barres (conservation du plus r√©cent via `last_modified_t`) et nettoyage des unit√©s (sel/sodium, kcal/kj).


* 
**Gold (Mod√©lisation)** : Structuration en sch√©ma en √©toile avec gestion de l'historisation **SCD Type 2** pour les produits.



## üìä Mod√®le de Donn√©es (Datamart)

Le datamart est compos√© des tables suivantes:

* 
**Faits** : `fact_nutrition_snapshot` (mesures pour 100g, scores, indices de compl√©tude).


* **Dimensions** :
* 
`dim_product` (SCD2 : `effective_from`, `is_current`).


* 
`dim_brand` (Marques).


* 
`dim_category` (Hi√©rarchie des cat√©gories).


* 
`dim_time` (Granularit√© temporelle : ISO week, mois, ann√©e).





## üõ°Ô∏è R√®gles de Qualit√© & M√©triques

Le pipeline int√®gre des contr√¥les rigoureux:

* 
**Unicit√©** : Un seul produit courant (`is_current = 1`) par code-barres.


* 
**Compl√©tude pond√©r√©e** : Score calcul√© sur la pr√©sence du nom, de la marque, et des nutriments cl√©s.


* 
**Contr√¥le des bornes** : Validation des valeurs nutritionnelles (ex: 0 ‚â§ sucre ‚â§ 100).


* 
**Reporting** : Export d'un fichier JSON de m√©triques apr√®s chaque run (nb produits filtr√©s, taux d'anomalies).



## üöÄ Installation et Utilisation

### Pr√©requis

* 
**Apache Spark** (PySpark).


* 
**MySQL 8**.


* 
**Connector JDBC MySQL** (`mysql-connector-j-8.x.jar`).



### Lancement de l'ETL

```bash
spark-submit --jars path/to/mysql-connector-j.jar main.py

```

### G√©n√©ration des KPIs

Une fois le chargement termin√©, la table `all_kpi` permet d'ex√©cuter les requ√™tes analytiques:

* Top 10 marques par Nutri-Score A/B.


* √âvolution hebdomadaire de la compl√©tude.


* Heatmap des sucres par cat√©gorie.



## üìÅ Structure du Repository

* 
`/etl` : Code PySpark (Ingestion, Transformation, Ingestion JDBC).


* 
`/sql` : Scripts DDL de cr√©ation des tables et requ√™tes analytiques.


* 
`/docs` : Dictionnaire des donn√©es et sch√©mas d'architecture.


* 
`/conf` : Param√®tres de connexion base de donn√©es.
Voici la **Note d'Architecture** d√©taill√©e pour votre projet, structur√©e pour r√©pondre aux exigences acad√©miques du niveau M1.

---

## Note d‚ÄôArchitecture : Pipeline OFF Nutrition & Qualit√©

### 1. Choix Technologiques

* 
**Moteur d'ex√©cution :** **Apache Spark (PySpark)** a √©t√© choisi pour sa capacit√© √† traiter des donn√©es massives (Big Data) de mani√®re distribu√©e, r√©pondant ainsi √† la sp√©cificit√© de l'exercice.


* 
**Stockage Cible :** **MySQL 8** via le connecteur **JDBC**, structur√© en Datamart pour optimiser les performances des requ√™tes analytiques SQL.


* 
**Langage :** **Python** pour la flexibilit√© de ses biblioth√®ques de manipulation de donn√©es et sa compatibilit√© native avec Spark.



### 2. Strat√©gie d'Architecture (M√©daillon)

L'int√©gration suit une progression de donn√©es en trois √©tapes pour garantir la tra√ßabilit√© et la qualit√©:

* 
**Couche Bronze (Ingestion) :** Lecture du fichier source (CSV/JSON) avec un **sch√©ma explicite** pour garantir la robustesse du pipeline en production.


* 
**Couche Silver (Conformation) :** * Nettoyage des donn√©es : suppression des doublons par code-barres en conservant l'enregistrement le plus r√©cent via `last_modified_t`.


* Normalisation : harmonisation des unit√©s (sel/sodium) et filtrage des valeurs aberrantes (0 ‚â§ nutriments ‚â§ 100).




* 
**Couche Gold (Mod√©lisation) :** Passage d'un format plat √† un **sch√©ma en √©toile** pour l'analyse d√©cisionnelle.



### 3. Mod√©lisation du Datamart

Le mod√®le repose sur une table de faits centrale et plusieurs dimensions descriptives:

* 
**Table de Faits (`fact_nutrition_snapshot`) :** Contient les m√©triques nutritionnelles (√©nergies, sucres, graisses) et le score de compl√©tude calcul√©.


* **Dimensions :**
* 
**dim_product :** Impl√©mentation du **SCD Type 2** (Slowly Changing Dimension) pour historiser les changements de produits (hash des attributs, `effective_from`, `is_current`).


* 
**dim_time :** Permet une analyse temporelle √† la semaine (ISO week) et au jour.


* 
**dim_brand & dim_category :** Normalisation des r√©f√©rentiels pour faciliter les classements par marque ou cat√©gorie.





### 4. Gestion de la Qualit√© (Data Quality)

La qualit√© est mesur√©e √† chaque run et export√©e sous format JSON:

* 
**Compl√©tude pond√©r√©e :** Calcul d'un score bas√© sur la pr√©sence des champs critiques (Nom, Nutriments, Cat√©gorie, Marque).


* 
**D√©tection d'anomalies :** Identification automatique des valeurs hors bornes (ex: sel > 25g) stock√©es dans le champ `quality_issues_json`.



### 5. Strat√©gie de Chargement (Upsert)

Pour garantir l'**idempotence** (capacit√© √† rejouer le script sans cr√©er de doublons), nous utilisons la strat√©gie suivante:

* **Dimensions :** Mode `append` avec gestion des cl√©s naturelles uniques.
* **Faits :** Chargement par snapshot quotidien utilisant des `time_sk` pour √©viter les recouvrements.

---

